{
    "task_name": "long_term_forecast",
    "is_training": 1,
    "model_id": "ETTh1_512_96",
    "model_comment": "TimeLLM-ETTh1-GPT2",
    "model": "TimeLLM",
    "seed": 2021,
    "data": "ETTh1",
    "root_path": "./dataset/ETT-small/",
    "data_path": "ETTh1.csv",
    "features": "M",
    "target": "OT",
    "loader": "modal",
    "freq": "h",
    "checkpoints": "./checkpoints/",
    "seq_len": 512,
    "label_len": 48,
    "pred_len": 96,
    "seasonal_patterns": "Monthly",
    "enc_in": 7,
    "dec_in": 7,
    "c_out": 7,
    "d_model": 32,
    "n_heads": 8,
    "e_layers": 2,
    "d_layers": 1,
    "d_ff": 128,
    "moving_avg": 25,
    "factor": 3,
    "dropout": 0.1,
    "embed": "timeF",
    "activation": "gelu",
    "output_attention": false,
    "patch_len": 16,
    "stride": 8,
    "prompt_domain": 0,
    "llm_model": "GPT2",
    "llm_dim": 768,
    "llm_cache_dir": "/home/nl/disk_8T/lys/cache/huggingface",
    "num_workers": 0,
    "itr": 1,
    "train_epochs": 15,
    "align_epochs": 10,
    "batch_size": 8,
    "eval_batch_size": 8,
    "patience": 10,
    "learning_rate": 0.01,
    "des": "Exp",
    "loss": "MSE",
    "lradj": "type1",
    "pct_start": 0.2,
    "use_amp": false,
    "llm_layers": 6,
    "percent": 100,
    "use_causal": true,
    "causal_cache_dir": "./causal_results",
    "causal_tau_max": 4,
    "causal_pc_alpha": 0.05,
    "causal_top_k": 10,
    "use_causal_loss": false,
    "causal_loss_weight": 0.1
}